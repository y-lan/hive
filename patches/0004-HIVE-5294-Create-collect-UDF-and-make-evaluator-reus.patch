From ab1a5217a9434a2576c96224805eba1c463cf2b5 Mon Sep 17 00:00:00 2001
From: yuyang-lan <yuyang.lan@gree.net>
Date: Tue, 4 Mar 2014 16:34:34 +0900
Subject: [PATCH] [HIVE-5294] Create collect UDF and make evaluator reusable

---
 .../hadoop/hive/ql/exec/FunctionRegistry.java      |   1 +
 .../ql/udf/generic/GenericUDAFCollectList.java     |  34 +++++
 .../hive/ql/udf/generic/GenericUDAFCollectSet.java | 117 +---------------
 .../generic/GenericUDAFMkCollectionEvaluator.java  | 147 +++++++++++++++++++++
 .../test/queries/clientpositive/udaf_collect_set.q |  11 ++
 .../results/clientpositive/show_functions.q.out    |   2 +
 .../results/clientpositive/udaf_collect_set.q.out  |  74 +++++++++++
 7 files changed, 272 insertions(+), 114 deletions(-)
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectList.java
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
index ffd2149..669c5ca 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java
@@ -383,6 +383,7 @@
     registerGenericUDAF("histogram_numeric", new GenericUDAFHistogramNumeric());
     registerGenericUDAF("percentile_approx", new GenericUDAFPercentileApprox());
     registerGenericUDAF("collect_set", new GenericUDAFCollectSet());
+    registerGenericUDAF("collect_list", new GenericUDAFCollectList());
 
     registerGenericUDAF("ngrams", new GenericUDAFnGrams());
     registerGenericUDAF("context_ngrams", new GenericUDAFContextNGrams());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectList.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectList.java
new file mode 100644
index 0000000..b617f35
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectList.java
@@ -0,0 +1,34 @@
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.hive.ql.exec.Description;
+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
+import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.BufferType;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+
+@Description(name = "collect_list", value = "_FUNC_(x) - Returns a list of objects with duplicates")
+public class GenericUDAFCollectList extends AbstractGenericUDAFResolver {
+
+  static final Log LOG = LogFactory.getLog(GenericUDAFCollectList.class.getName());
+
+  public GenericUDAFCollectList() {
+  }
+
+  @Override
+  public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
+      throws SemanticException {
+    if (parameters.length != 1) {
+      throw new UDFArgumentTypeException(parameters.length - 1,
+          "Exactly one argument is expected.");
+    }
+    if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
+      throw new UDFArgumentTypeException(0,
+          "Only primitive type arguments are accepted but "
+          + parameters[0].getTypeName() + " was passed as parameter 1.");
+    }
+    return new GenericUDAFMkCollectionEvaluator(BufferType.LIST);
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectSet.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectSet.java
index 5efc14b..6dc424a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectSet.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFCollectSet.java
@@ -17,21 +17,13 @@
  */
 package org.apache.hadoop.hive.ql.udf.generic;
 
-import java.util.ArrayList;
-import java.util.HashSet;
-import java.util.Set;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.exec.Description;
 import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.BufferType;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
 
 /**
@@ -41,126 +33,23 @@
 public class GenericUDAFCollectSet extends AbstractGenericUDAFResolver {
 
   static final Log LOG = LogFactory.getLog(GenericUDAFCollectSet.class.getName());
-  
+
   public GenericUDAFCollectSet() {
   }
 
   @Override
   public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters)
       throws SemanticException {
-
     if (parameters.length != 1) {
       throw new UDFArgumentTypeException(parameters.length - 1,
           "Exactly one argument is expected.");
     }
-
     if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
       throw new UDFArgumentTypeException(0,
           "Only primitive type arguments are accepted but "
           + parameters[0].getTypeName() + " was passed as parameter 1.");
     }
-
-    return new GenericUDAFMkSetEvaluator();
+    return new GenericUDAFMkCollectionEvaluator(BufferType.SET);
   }
 
-  public static class GenericUDAFMkSetEvaluator extends GenericUDAFEvaluator {
-    
-    // For PARTIAL1 and COMPLETE: ObjectInspectors for original data
-    private PrimitiveObjectInspector inputOI;
-    // For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations (list
-    // of objs)
-    private transient StandardListObjectInspector loi;
-    
-    private transient StandardListObjectInspector internalMergeOI;
-    
-    @Override
-    public ObjectInspector init(Mode m, ObjectInspector[] parameters)
-        throws HiveException {
-      super.init(m, parameters);
-      // init output object inspectors
-      // The output of a partial aggregation is a list
-      if (m == Mode.PARTIAL1) {
-        inputOI = (PrimitiveObjectInspector) parameters[0];
-        return ObjectInspectorFactory
-            .getStandardListObjectInspector((PrimitiveObjectInspector) ObjectInspectorUtils
-                .getStandardObjectInspector(inputOI));
-      } else {
-        if (!(parameters[0] instanceof StandardListObjectInspector)) {
-          //no map aggregation.
-          inputOI = (PrimitiveObjectInspector)  ObjectInspectorUtils
-          .getStandardObjectInspector(parameters[0]);
-          return (StandardListObjectInspector) ObjectInspectorFactory
-              .getStandardListObjectInspector(inputOI);
-        } else {
-          internalMergeOI = (StandardListObjectInspector) parameters[0];
-          inputOI = (PrimitiveObjectInspector) internalMergeOI.getListElementObjectInspector();
-          loi = (StandardListObjectInspector) ObjectInspectorUtils.getStandardObjectInspector(internalMergeOI);          
-          return loi;
-        }
-      }
-    }
-    
-    static class MkArrayAggregationBuffer extends AbstractAggregationBuffer {
-      Set<Object> container;
-    }
-    
-    @Override
-    public void reset(AggregationBuffer agg) throws HiveException {
-      ((MkArrayAggregationBuffer) agg).container = new HashSet<Object>();
-    }
-    
-    @Override
-    public AggregationBuffer getNewAggregationBuffer() throws HiveException {
-      MkArrayAggregationBuffer ret = new MkArrayAggregationBuffer();
-      reset(ret);
-      return ret;
-    }
-
-    //mapside
-    @Override
-    public void iterate(AggregationBuffer agg, Object[] parameters)
-        throws HiveException {
-      assert (parameters.length == 1);
-      Object p = parameters[0];
-
-      if (p != null) {
-        MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
-        putIntoSet(p, myagg);
-      }
-    }
-
-    //mapside
-    @Override
-    public Object terminatePartial(AggregationBuffer agg) throws HiveException {
-      MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
-      ArrayList<Object> ret = new ArrayList<Object>(myagg.container.size());
-      ret.addAll(myagg.container);
-      return ret;
-    }
-
-    @Override
-    public void merge(AggregationBuffer agg, Object partial)
-        throws HiveException {
-      MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
-      ArrayList<Object> partialResult = (ArrayList<Object>) internalMergeOI.getList(partial);
-      for(Object i : partialResult) {
-        putIntoSet(i, myagg);
-      }
-    }
-    
-    @Override
-    public Object terminate(AggregationBuffer agg) throws HiveException {
-      MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
-      ArrayList<Object> ret = new ArrayList<Object>(myagg.container.size());
-      ret.addAll(myagg.container);
-      return ret;
-    }
-    
-    private void putIntoSet(Object p, MkArrayAggregationBuffer myagg) {
-      Object pCopy = ObjectInspectorUtils.copyToStandardObject(p,
-          this.inputOI);
-      myagg.container.add(pCopy);
-    }
-  }
-  
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java
new file mode 100644
index 0000000..43c76d1
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFMkCollectionEvaluator.java
@@ -0,0 +1,147 @@
+package org.apache.hadoop.hive.ql.udf.generic;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.List;
+
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;
+import org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector;
+
+import com.esotericsoftware.minlog.Log;
+
+public class GenericUDAFMkCollectionEvaluator extends GenericUDAFEvaluator {
+
+  enum BufferType { SET, LIST }
+
+  // For PARTIAL1 and COMPLETE: ObjectInspectors for original data
+  private PrimitiveObjectInspector inputOI;
+  // For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations (list
+  // of objs)
+  private transient StandardListObjectInspector loi;
+
+  private transient StandardListObjectInspector internalMergeOI;
+
+  private BufferType bufferType;
+
+  //needed by kyro
+  public GenericUDAFMkCollectionEvaluator(){
+
+  }
+
+  public GenericUDAFMkCollectionEvaluator(BufferType bufferType){
+    this.bufferType = bufferType;
+  }
+
+  @Override
+  public ObjectInspector init(Mode m, ObjectInspector[] parameters)
+      throws HiveException {
+    super.init(m, parameters);
+    // init output object inspectors
+    // The output of a partial aggregation is a list
+    if (m == Mode.PARTIAL1) {
+      inputOI = (PrimitiveObjectInspector) parameters[0];
+      return ObjectInspectorFactory
+          .getStandardListObjectInspector((PrimitiveObjectInspector) ObjectInspectorUtils
+              .getStandardObjectInspector(inputOI));
+    } else {
+      if (!(parameters[0] instanceof StandardListObjectInspector)) {
+        //no map aggregation.
+        inputOI = (PrimitiveObjectInspector)  ObjectInspectorUtils
+        .getStandardObjectInspector(parameters[0]);
+        return (StandardListObjectInspector) ObjectInspectorFactory
+            .getStandardListObjectInspector(inputOI);
+      } else {
+        internalMergeOI = (StandardListObjectInspector) parameters[0];
+        inputOI = (PrimitiveObjectInspector) internalMergeOI.getListElementObjectInspector();
+        loi = (StandardListObjectInspector) ObjectInspectorUtils.getStandardObjectInspector(internalMergeOI);
+        return loi;
+      }
+    }
+  }
+
+
+  class MkArrayAggregationBuffer extends AbstractAggregationBuffer {
+
+    private Collection<Object> container;
+
+    public MkArrayAggregationBuffer() {
+      if (bufferType == BufferType.LIST){
+        container = new ArrayList<Object>();
+      } else if(bufferType == BufferType.SET){
+        container = new HashSet<Object>();
+      } else {
+        throw new RuntimeException("Buffer type unknown");
+      }
+    }
+  }
+
+  @Override
+  public void reset(AggregationBuffer agg) throws HiveException {
+    ((MkArrayAggregationBuffer) agg).container.clear();
+  }
+
+  @Override
+  public AggregationBuffer getNewAggregationBuffer() throws HiveException {
+    MkArrayAggregationBuffer ret = new MkArrayAggregationBuffer();
+    return ret;
+  }
+
+  //mapside
+  @Override
+  public void iterate(AggregationBuffer agg, Object[] parameters)
+      throws HiveException {
+    assert (parameters.length == 1);
+    Object p = parameters[0];
+
+    if (p != null) {
+      MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
+      putIntoCollection(p, myagg);
+    }
+  }
+
+  //mapside
+  @Override
+  public Object terminatePartial(AggregationBuffer agg) throws HiveException {
+    MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
+    List<Object> ret = new ArrayList<Object>(myagg.container.size());
+    ret.addAll(myagg.container);
+    return ret;
+  }
+
+  @Override
+  public void merge(AggregationBuffer agg, Object partial)
+      throws HiveException {
+    MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
+    List<Object> partialResult = (ArrayList<Object>) internalMergeOI.getList(partial);
+    for(Object i : partialResult) {
+      putIntoCollection(i, myagg);
+    }
+  }
+
+  @Override
+  public Object terminate(AggregationBuffer agg) throws HiveException {
+    MkArrayAggregationBuffer myagg = (MkArrayAggregationBuffer) agg;
+    List<Object> ret = new ArrayList<Object>(myagg.container.size());
+    ret.addAll(myagg.container);
+    return ret;
+  }
+
+  private void putIntoCollection(Object p, MkArrayAggregationBuffer myagg) {
+    Object pCopy = ObjectInspectorUtils.copyToStandardObject(p,  this.inputOI);
+    myagg.container.add(pCopy);
+  }
+
+  public BufferType getBufferType() {
+    return bufferType;
+  }
+
+  public void setBufferType(BufferType bufferType) {
+    this.bufferType = bufferType;
+  }
+
+}
diff --git a/ql/src/test/queries/clientpositive/udaf_collect_set.q b/ql/src/test/queries/clientpositive/udaf_collect_set.q
index 45aaa02..04bea32 100644
--- a/ql/src/test/queries/clientpositive/udaf_collect_set.q
+++ b/ql/src/test/queries/clientpositive/udaf_collect_set.q
@@ -1,6 +1,9 @@
 DESCRIBE FUNCTION collect_set;
 DESCRIBE FUNCTION EXTENDED collect_set;
 
+DESCRIBE FUNCTION collect_list;
+DESCRIBE FUNCTION EXTENDED collect_list;
+
 set hive.map.aggr = false;
 set hive.groupby.skewindata = false;
 
@@ -8,6 +11,10 @@ SELECT key, collect_set(value)
 FROM src
 GROUP BY key ORDER BY key limit 20;
 
+SELECT key, collect_list(value)
+FROM src
+GROUP BY key ORDER by key limit 20;
+
 set hive.map.aggr = true;
 set hive.groupby.skewindata = false;
 
@@ -15,6 +22,10 @@ SELECT key, collect_set(value)
 FROM src
 GROUP BY key ORDER BY key limit 20;
 
+SELECT key, collect_list(value)
+FROM src
+GROUP BY key ORDER BY key limit 20;
+
 set hive.map.aggr = false;
 set hive.groupby.skewindata = true;
 
diff --git a/ql/src/test/results/clientpositive/show_functions.q.out b/ql/src/test/results/clientpositive/show_functions.q.out
index eab00d7..f01827d 100644
--- a/ql/src/test/results/clientpositive/show_functions.q.out
+++ b/ql/src/test/results/clientpositive/show_functions.q.out
@@ -36,6 +36,7 @@ case
 ceil
 ceiling
 coalesce
+collect_list
 collect_set
 compute_stats
 concat
@@ -202,6 +203,7 @@ case
 ceil
 ceiling
 coalesce
+collect_list
 collect_set
 compute_stats
 concat
diff --git a/ql/src/test/results/clientpositive/udaf_collect_set.q.out b/ql/src/test/results/clientpositive/udaf_collect_set.q.out
index 30a6a27..42f2d9d 100644
--- a/ql/src/test/results/clientpositive/udaf_collect_set.q.out
+++ b/ql/src/test/results/clientpositive/udaf_collect_set.q.out
@@ -8,6 +8,16 @@ PREHOOK: type: DESCFUNCTION
 POSTHOOK: query: DESCRIBE FUNCTION EXTENDED collect_set
 POSTHOOK: type: DESCFUNCTION
 collect_set(x) - Returns a set of objects with duplicate elements eliminated
+PREHOOK: query: DESCRIBE FUNCTION collect_list
+PREHOOK: type: DESCFUNCTION
+POSTHOOK: query: DESCRIBE FUNCTION collect_list
+POSTHOOK: type: DESCFUNCTION
+collect_list(x) - Returns a list of objects with duplicates
+PREHOOK: query: DESCRIBE FUNCTION EXTENDED collect_list
+PREHOOK: type: DESCFUNCTION
+POSTHOOK: query: DESCRIBE FUNCTION EXTENDED collect_list
+POSTHOOK: type: DESCFUNCTION
+collect_list(x) - Returns a list of objects with duplicates
 PREHOOK: query: SELECT key, collect_set(value)
 FROM src
 GROUP BY key ORDER BY key limit 20
@@ -40,6 +50,38 @@ POSTHOOK: Input: default@src
 128	["val_128"]
 129	["val_129"]
 131	["val_131"]
+PREHOOK: query: SELECT key, collect_list(value)
+FROM src
+GROUP BY key ORDER by key limit 20
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT key, collect_list(value)
+FROM src
+GROUP BY key ORDER by key limit 20
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+0	["val_0","val_0","val_0"]
+10	["val_10"]
+100	["val_100","val_100"]
+103	["val_103","val_103"]
+104	["val_104","val_104"]
+105	["val_105"]
+11	["val_11"]
+111	["val_111"]
+113	["val_113","val_113"]
+114	["val_114"]
+116	["val_116"]
+118	["val_118","val_118"]
+119	["val_119","val_119","val_119"]
+12	["val_12","val_12"]
+120	["val_120","val_120"]
+125	["val_125","val_125"]
+126	["val_126"]
+128	["val_128","val_128","val_128"]
+129	["val_129","val_129"]
+131	["val_131"]
 PREHOOK: query: SELECT key, collect_set(value)
 FROM src
 GROUP BY key ORDER BY key limit 20
@@ -72,6 +114,38 @@ POSTHOOK: Input: default@src
 128	["val_128"]
 129	["val_129"]
 131	["val_131"]
+PREHOOK: query: SELECT key, collect_list(value)
+FROM src
+GROUP BY key ORDER BY key limit 20
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT key, collect_list(value)
+FROM src
+GROUP BY key ORDER BY key limit 20
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+0	["val_0","val_0","val_0"]
+10	["val_10"]
+100	["val_100","val_100"]
+103	["val_103","val_103"]
+104	["val_104","val_104"]
+105	["val_105"]
+11	["val_11"]
+111	["val_111"]
+113	["val_113","val_113"]
+114	["val_114"]
+116	["val_116"]
+118	["val_118","val_118"]
+119	["val_119","val_119","val_119"]
+12	["val_12","val_12"]
+120	["val_120","val_120"]
+125	["val_125","val_125"]
+126	["val_126"]
+128	["val_128","val_128","val_128"]
+129	["val_129","val_129"]
+131	["val_131"]
 PREHOOK: query: SELECT key, collect_set(value)
 FROM src
 GROUP BY key ORDER BY key limit 20
-- 
1.8.3.4 (Apple Git-47)

